# -*- coding: utf-8 -*-
"""ML_predicts_youtube_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WVcz6kYfRCkVcFTGznIo8eaY5ycs0VDe
"""

import pandas as pd

data = pd.read_csv("youtube_data.csv")
data.describe()

# You only need the other line if you find empty cells in the table
# data = data.dropna(axis=0)
# There are also other methods with dealing with missing data - imputing (putting in averages) and removing the entire colummn

y = data.views
features = ['duration','bitrate','frame rate','likes','comments']
X = data[features]

# break up the data into training data and testing data
# remember - the random state number isn't important other than it is used to make reproducible results
# if other functions call for a random_state number, they do not have to match, they only have to remain fixed for reproducible results
from sklearn.model_selection import train_test_split
train_X, val_X, train_y, val_y = train_test_split(X,y,random_state = 1)

# Compare the preformance of different ML models



# First, the Decision Tree without leaf optimization
from sklearn.tree import DecisionTreeRegressor
model1 = DecisionTreeRegressor(random_state = 2)
model1.fit(train_X, train_y)

from sklearn.metrics import mean_absolute_error

print("The Mean Absolute Error for the Decision Tree ML model without leaf optimization is",mean_absolute_error(model1.predict(val_X),val_y))

#Second, the Decision Tree with leaf optimization
def get_mae(leaves, train_X, val_X, train_y, val_y):
  model2 = DecisionTreeRegressor(max_leaf_nodes=leaves, random_state=55)
  model2.fit(train_X, train_y)
  return mean_absolute_error(model2.predict(val_X), val_y)

leaves = [50,500,5000]
lowest = get_mae(5,train_X, val_X, train_y, val_y)
for i in leaves:
  if get_mae(i,train_X, val_X, train_y, val_y)<lowest:
    lowest = get_mae(i,train_X, val_X, train_y, val_y)
print("The Mean Absolute Error for the Decision Tree ML model with leaf optimization is",lowest)

#Third, the Random Forest
from sklearn.ensemble import RandomForestRegressor
model3 = RandomForestRegressor(n_estimators=50, random_state = 5)
model3.fit(train_X, train_y)
print("The Mean Absolute Error for the Random Tree ML model is",mean_absolute_error(model3.predict(val_X),val_y))

#Fourth XGBoost
from xgboost import XGBRegressor
model4 = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, verbosity = 1, early_stopping_rounds=10)
model4.fit(train_X, train_y,eval_set=[(val_X, val_y)])
print("The Mean Absolute Error for the XGBoost model is", mean_absolute_error(model4.predict(val_X), val_y))